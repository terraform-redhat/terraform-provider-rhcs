---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "rhcs_cluster_autoscaler Resource - terraform-provider-rhcs"
subcategory: ""
description: |-
  Cluster-wide autoscaling configuration.
---

# rhcs_cluster_autoscaler (Resource)

Cluster-wide autoscaling configuration.

## Example Usage

```terraform
resource "rhcs_cluster_autoscaler" "cluster_autoscaler" {
  cluster                       = "cluster-id-123"
  max_pod_grace_period          = 600
  pod_priority_threshold        = -10
  max_node_provision_time       = "15m"
  balance_similar_node_groups   = true
  balancing_ignored_labels      = ["example-label"]
  skip_nodes_with_local_storage = false
  log_verbosity                 = 4

  resource_limits = {
    max_nodes_total = 5
    cores = {
      max = 11520
      min = 1
    }
    memory = {
      max = 230400
      min = 1
    }
    gpu = {
      type = "nvidia.com/gpu" # or amd.com/gpu
      range = {
        min = 0
        max = 10
      }
    }
  }
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `cluster` (String) Identifier of the cluster.After the creation of the resource, it is not possible to update the attribute value.

### Optional

- `balance_similar_node_groups` (Boolean) Automatically identify node groups with the same instance type and the same set of labels and try to keep the respective sizes of those node groups balanced.
- `balancing_ignored_labels` (List of String) This option specifies labels that cluster autoscaler should ignore when considering node group similarity. For example, if you have nodes with 'topology.ebs.csi.aws.com/zone' label, you can add name of this label here to prevent cluster autoscaler from splitting nodes into different node groups based on its value.
- `ignore_daemonsets_utilization` (Boolean) Should cluster-autoscaler ignore DaemonSet pods when calculating resource utilization for scaling down. false by default
- `log_verbosity` (Number) Sets the autoscaler log level. Default value is 1, level 4 is recommended for DEBUGGING and level 6 will enable almost everything.
- `max_node_provision_time` (String) Maximum time cluster-autoscaler waits for node to be provisioned.
- `max_pod_grace_period` (Number) Gives pods graceful termination time before scaling down.
- `pod_priority_threshold` (Number) To allow users to schedule 'best-effort' pods, which shouldn't trigger Cluster Autoscaler actions, but only run when there are spare resources available.
- `resource_limits` (Attributes) Constraints of autoscaling resources. (see [below for nested schema](#nestedatt--resource_limits))
- `scale_down` (Attributes) Configuration of scale down operation. (see [below for nested schema](#nestedatt--scale_down))
- `skip_nodes_with_local_storage` (Boolean) If true cluster autoscaler will never delete nodes with pods with local storage, e.g. EmptyDir or HostPath. true by default at autoscaler.

<a id="nestedatt--resource_limits"></a>
### Nested Schema for `resource_limits`

Optional:

- `cores` (Attributes) Minimum and maximum number of cores in cluster, in the format <min>:<max>. Cluster autoscaler will not scale the cluster beyond these numbers. (see [below for nested schema](#nestedatt--resource_limits--cores))
- `gpus` (Attributes List) Minimum and maximum number of different GPUs in cluster, in the format <gpu_type>:<min>:<max>. Cluster autoscaler will not scale the cluster beyond these numbers. Can be passed multiple times. (see [below for nested schema](#nestedatt--resource_limits--gpus))
- `max_nodes_total` (Number) Maximum number of nodes in all node groups. Cluster autoscaler will not grow the cluster beyond this number.See OpenShift documentation for maximum allowed values: https://github.com/openshift/openshift-docs/blob/main/cloud_experts_tutorials/cloud-experts-getting-started/cloud-experts-getting-started-what-is-rosa.adoc
- `memory` (Attributes) Minimum and maximum number of gigabytes of memory in cluster, in the format <min>:<max>. Cluster autoscaler will not scale the cluster beyond these numbers. (see [below for nested schema](#nestedatt--resource_limits--memory))

<a id="nestedatt--resource_limits--cores"></a>
### Nested Schema for `resource_limits.cores`

Required:

- `max` (Number)
- `min` (Number)


<a id="nestedatt--resource_limits--gpus"></a>
### Nested Schema for `resource_limits.gpus`

Required:

- `range` (Attributes) limit number of GPU type (see [below for nested schema](#nestedatt--resource_limits--gpus--range))
- `type` (String)

<a id="nestedatt--resource_limits--gpus--range"></a>
### Nested Schema for `resource_limits.gpus.range`

Required:

- `max` (Number)
- `min` (Number)



<a id="nestedatt--resource_limits--memory"></a>
### Nested Schema for `resource_limits.memory`

Required:

- `max` (Number)
- `min` (Number)



<a id="nestedatt--scale_down"></a>
### Nested Schema for `scale_down`

Optional:

- `delay_after_add` (String) How long after scale up that scale down evaluation resumes.
- `delay_after_delete` (String) How long after node deletion that scale down evaluation resumes.
- `delay_after_failure` (String) How long after scale down failure that scale down evaluation resumes.
- `enabled` (Boolean) Should cluster-autoscaler scale down the cluster.
- `unneeded_time` (String) How long a node should be unneeded before it is eligible for scale down.
- `utilization_threshold` (String) Node utilization level, defined as sum of requested resources divided by capacity, below which a node can be considered for scale down.
